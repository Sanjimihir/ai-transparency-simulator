AI Transparency Simulator
Loan Decision Explainability & Trust Simulation
ğŸ“Œ Overview

The AI Transparency Simulator is an interactive Streamlit-based application designed to demonstrate AI explainability, transparency, and user trust in automated loan approval systems.

The project simulates how different explanation mechanismsâ€”ranging from no explanation to SHAP-based explanations and counterfactual reasoningâ€”affect user trust in AI-driven financial decisions.

This tool is intended for AI ethics research, finance analytics, explainable AI (XAI) education, and transparency studies.

ğŸ¯ Objectives

Simulate automated loan approval decisions

Provide multiple levels of model explainability

Allow users to interact with applicant data

Measure user trust under different explanation modes

Demonstrate responsible AI practices in the finance domain

ğŸ§  Explanation Modes Supported

No Explanation (Control Group)

Only shows the model decision and probability

Basic Explanation

Displays top contributing features using model feature importance

Detailed Explanation (SHAP)

Uses SHAP values to show feature-level contribution to predictions

Counterfactual Explanations

Suggests minimal feature changes required to flip the decision outcome

ğŸ—ï¸ System Architecture
User Input
   â†“
Streamlit Interface
   â†“
ML Pipeline (Random Forest)
   â†“
Prediction (Approve / Reject)
   â†“
Explanation Layer
   â”œâ”€â”€ Feature Importance
   â”œâ”€â”€ SHAP Values
   â””â”€â”€ Counterfactual Generator
   â†“
Trust Feedback Collection

ğŸ§ª Dataset

German Credit Dataset

Used for binary classification:

1 â†’ Creditworthy

0 â†’ Non-creditworthy

Commonly used in credit-risk and fairness research

ğŸ› ï¸ Tech Stack
Category	Tools
Language	Python
Frontend	Streamlit
ML Model	Random Forest
Explainability	SHAP
Data Handling	Pandas, NumPy
Visualization	Matplotlib
Model Storage	Joblib
ğŸ“‚ Project Structure
ai-transparency-simulator/
â”‚
â”œâ”€â”€ app.py                  # Main Streamlit application
â”œâ”€â”€ model.joblib             # Trained ML pipeline
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ README.md                # Project documentation
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ german_credit.csv    # Dataset
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_utils.py        # Data loading & preprocessing
â”‚   â”œâ”€â”€ model.py             # Model training & persistence
â”‚   â”œâ”€â”€ explainer.py         # SHAP & explanation wrapper
â”‚   â””â”€â”€ counterfactuals.py   # Counterfactual generation logic

ğŸš€ How to Run the Project Locally
1ï¸âƒ£ Clone the Repository
git clone https://github.com/YOUR_USERNAME/ai-transparency-simulator.git
cd ai-transparency-simulator

2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

3ï¸âƒ£ Run the Application
streamlit run app.py


The app will open automatically in your browser.

ğŸ“Š Model Evaluation

Metric Used: ROC-AUC

Displayed dynamically inside the application

Helps validate predictive performance alongside explainability

ğŸ” Key Features

Interactive applicant selection

Manual feature input option

Real-time prediction probabilities

Visual SHAP plots

Counterfactual recommendations

Trust score capture and storage

Robust error handling for small datasets

âš–ï¸ Ethical & Research Relevance

This project aligns with modern Responsible AI principles:

Transparency

Interpretability

User trust

Human-in-the-loop decision-making

It can be used as:

A teaching tool for XAI concepts

A prototype for ethical AI systems in finance

A foundation for academic or applied research

ğŸ“ˆ Possible Extensions

Fairness metrics (bias detection across demographics)

Model comparison (Logistic Regression vs RF)

Advanced counterfactual optimization

User study analytics dashboard

Integration with real-world financial datasets
